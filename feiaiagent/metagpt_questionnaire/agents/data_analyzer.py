# -*- coding: utf-8 -*-
"""
æ•°æ®åˆ†ææ™ºèƒ½ä½“
è´Ÿè´£åˆ†æé—®å·æ•°æ®ã€è¯†åˆ«æ¨¡å¼ã€æä¾›æ´å¯Ÿ
"""

import logging
import json
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from collections import defaultdict, Counter

from .base_agent import BaseAgent, register_agent
from ..models.questionnaire import UserResponse, Question, Questionnaire
from ..prompts.design_prompts import AnalysisPrompts

logger = logging.getLogger(__name__)

@register_agent
class DataAnalyzerAgent(BaseAgent):
    """æ•°æ®åˆ†ææ™ºèƒ½ä½“"""
    
    def __init__(self):
        super().__init__(
            name="æ•°æ®åˆ†æä¸“å®¶",
            description="ä¸“ä¸šåˆ†æé—®å·æ•°æ®çš„æ™ºèƒ½ä½“ï¼Œæ“…é•¿æ•°æ®æ´å¯Ÿå’Œæ¨¡å¼è¯†åˆ«",
            expertise=["æ•°æ®åˆ†æ", "ç»Ÿè®¡å­¦", "æ¨¡å¼è¯†åˆ«", "æ•°æ®å¯è§†åŒ–", "æ´å¯Ÿå‘ç°"]
        )
        self.analysis_history: List[Dict[str, Any]] = []
    
    async def process(self, input_data: Any) -> Any:
        """å¤„ç†æ•°æ®åˆ†æè¯·æ±‚"""
        if isinstance(input_data, dict):
            responses = input_data.get('responses', [])
            questionnaire = input_data.get('questionnaire')
            analysis_type = input_data.get('analysis_type', 'comprehensive')
            return await self.analyze_data(responses, questionnaire, analysis_type)
        elif isinstance(input_data, list):
            return await self.analyze_data(input_data)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥ç±»å‹: {type(input_data)}")
    
    async def analyze_data(self, responses: List[UserResponse], 
                          questionnaire: Optional[Questionnaire] = None,
                          analysis_type: str = 'comprehensive') -> Dict[str, Any]:
        """åˆ†æé—®å·æ•°æ®"""
        logger.info(f"ğŸ“Š {self.name} å¼€å§‹æ•°æ®åˆ†æï¼Œå›ç­”æ•°é‡: {len(responses)}")
        
        try:
            # åŸºç¡€ç»Ÿè®¡åˆ†æ
            basic_stats = self._analyze_basic_statistics(responses)
            
            # åˆ†ç±»åˆ†æ
            category_analysis = self._analyze_by_category(responses, questionnaire)
            
            # æ¨¡å¼è¯†åˆ«
            pattern_analysis = await self._identify_patterns(responses)
            
            # æ•°æ®è´¨é‡è¯„ä¼°
            quality_assessment = self._assess_data_quality(responses)
            
            # æ´å¯Ÿå‘ç°
            insights = await self._discover_insights(responses, questionnaire)
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            analysis_result = {
                "analysis_id": f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                "timestamp": datetime.now().isoformat(),
                "responses_count": len(responses),
                "analysis_type": analysis_type,
                "basic_statistics": basic_stats,
                "category_analysis": category_analysis,
                "pattern_analysis": pattern_analysis,
                "data_quality": quality_assessment,
                "insights": insights,
                "summary": self._generate_summary(basic_stats, insights)
            }
            
            # ä¿å­˜åˆ°å†å²è®°å½•
            self.analysis_history.append(analysis_result)
            
            logger.info(f"âœ… {self.name} æ•°æ®åˆ†æå®Œæˆ")
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ {self.name} æ•°æ®åˆ†æå¤±è´¥: {e}")
            return self._create_error_analysis_result(str(e))
    
    def _analyze_basic_statistics(self, responses: List[UserResponse]) -> Dict[str, Any]:
        """åŸºç¡€ç»Ÿè®¡åˆ†æ"""
        stats = {
            "total_responses": len(responses),
            "unique_questions": len(set(r.question_id for r in responses)),
            "response_timestamps": [],
            "completion_time": None,
            "response_distribution": {}
        }
        
        if responses:
            # æ—¶é—´åˆ†æ
            timestamps = [r.timestamp for r in responses if r.timestamp]
            if timestamps:
                stats["response_timestamps"] = [ts.isoformat() for ts in timestamps]
                if len(timestamps) > 1:
                    completion_time = max(timestamps) - min(timestamps)
                    stats["completion_time"] = completion_time.total_seconds()
            
            # å›ç­”åˆ†å¸ƒåˆ†æ
            question_ids = [r.question_id for r in responses]
            question_counter = Counter(question_ids)
            stats["response_distribution"] = dict(question_counter)
        
        return stats
    
    def _extract_min_questions(self, questionnaire: Any) -> List[tuple]:
        """ä»é—®å·å¯¹è±¡æˆ–å­—å…¸ä¸­æå– (question_id, category) åˆ—è¡¨"""
        result: List[tuple] = []
        try:
            if not questionnaire:
                return result
            # å¯¹è±¡å½¢å¼ï¼ˆæœ‰ questions å±æ€§ï¼‰
            if hasattr(questionnaire, "questions"):
                for q in getattr(questionnaire, "questions", []) or []:
                    try:
                        result.append((q.id, q.category))
                    except Exception:
                        continue
                return result
            # å­—å…¸å½¢å¼ï¼ˆæ¥è‡ª to_dict çš„ç»“æ„ï¼‰
            if isinstance(questionnaire, dict):
                for q in questionnaire.get("questions", []) or []:
                    try:
                        qid = q.get("id")
                        cat = q.get("category")
                        if qid and cat is not None:
                            result.append((qid, cat))
                    except Exception:
                        continue
                return result
        except Exception:
            return result
        return result

    def _analyze_by_category(self, responses: List[UserResponse], 
                           questionnaire: Any) -> Dict[str, Any]:
        """æŒ‰åˆ†ç±»åˆ†ææ•°æ®ï¼ˆå…¼å®¹å¯¹è±¡æˆ–dictï¼‰"""
        category_analysis: Dict[str, Any] = {}
        
        questions_min = self._extract_min_questions(questionnaire)
        if not questions_min:
            return category_analysis
        
        # æŒ‰åˆ†ç±»ç»„ç»‡é—®é¢˜ID
        questions_by_category: Dict[str, List[str]] = defaultdict(list)
        for qid, cat in questions_min:
            questions_by_category[str(cat)].append(qid)
        
        # åˆ†ææ¯ä¸ªåˆ†ç±»çš„å›ç­”æƒ…å†µ
        for category, qids in questions_by_category.items():
            qid_set = set(qids)
            category_responses = [r for r in responses if r.question_id in qid_set]
            if category_responses:
                category_stats = {
                    "question_count": len(qids),
                    "response_count": len(category_responses),
                    "completion_rate": (len(category_responses) / max(1, len(qids))),
                    "response_patterns": self._analyze_response_patterns(category_responses),
                    "common_answers": self._find_common_answers(category_responses)
                }
                category_analysis[category] = category_stats
        
        return category_analysis
    
    def _analyze_response_patterns(self, responses: List[UserResponse]) -> Dict[str, Any]:
        """åˆ†æå›ç­”æ¨¡å¼"""
        patterns = {
            "answer_types": {},
            "response_lengths": [],
            "confidence_scores": []
        }
        
        for response in responses:
            # åˆ†æç­”æ¡ˆç±»å‹
            answer_type = type(response.answer).__name__
            patterns["answer_types"][answer_type] = patterns["answer_types"].get(answer_type, 0) + 1
            
            # åˆ†æç­”æ¡ˆé•¿åº¦
            if isinstance(response.answer, str):
                patterns["response_lengths"].append(len(response.answer))
            
            # åˆ†æç½®ä¿¡åº¦
            if response.confidence is not None:
                patterns["confidence_scores"].append(response.confidence)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        if patterns["response_lengths"]:
            patterns["avg_response_length"] = sum(patterns["response_lengths"]) / len(patterns["response_lengths"])
            patterns["max_response_length"] = max(patterns["response_lengths"])
            patterns["min_response_length"] = min(patterns["response_lengths"])
        
        if patterns["confidence_scores"]:
            patterns["avg_confidence"] = sum(patterns["confidence_scores"]) / len(patterns["confidence_scores"])
            patterns["confidence_range"] = (min(patterns["confidence_scores"]), max(patterns["confidence_scores"]))
        
        return patterns
    
    def _find_common_answers(self, responses: List[UserResponse]) -> List[Dict[str, Any]]:
        """æŸ¥æ‰¾å¸¸è§ç­”æ¡ˆ"""
        answer_counter = Counter()
        
        for response in responses:
            answer_str = str(response.answer)
            answer_counter[answer_str] += 1
        
        # è¿”å›å‰5ä¸ªæœ€å¸¸è§ç­”æ¡ˆ
        common_answers = []
        for answer, count in answer_counter.most_common(5):
            common_answers.append({
                "answer": answer,
                "count": count,
                "percentage": (count / len(responses)) * 100
            })
        
        return common_answers
    
    async def _identify_patterns(self, responses: List[UserResponse]) -> Dict[str, Any]:
        """è¯†åˆ«æ•°æ®æ¨¡å¼"""
        logger.info(f"ğŸ” {self.name} å¼€å§‹æ¨¡å¼è¯†åˆ«")
        
        try:
            # è·å–æ¨¡å¼è¯†åˆ«æç¤ºè¯
            prompt = AnalysisPrompts.pattern_recognition_prompt([r.to_dict() for r in responses])
            
            # è°ƒç”¨LLMè¿›è¡Œæ¨¡å¼è¯†åˆ«
            llm_response = await self.call_llm(prompt)
            
            # è§£ææ¨¡å¼è¯†åˆ«ç»“æœ
            patterns = self._parse_pattern_analysis(llm_response)
            
            # å¦‚æœæ²¡æœ‰è¯†åˆ«åˆ°æ¨¡å¼ï¼Œä½¿ç”¨åŸºç¡€æ¨¡å¼åˆ†æ
            if not patterns:
                patterns = self._basic_pattern_analysis(responses)
            
            logger.info(f"âœ… æ¨¡å¼è¯†åˆ«å®Œæˆ: {len(patterns)} ä¸ªæ¨¡å¼")
            return patterns
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å¼è¯†åˆ«å¤±è´¥: {e}")
            return self._basic_pattern_analysis(responses)
    
    def _parse_pattern_analysis(self, llm_response: str) -> Dict[str, Any]:
        """è§£æLLMçš„æ¨¡å¼åˆ†æç»“æœ"""
        patterns = {
            "identified_patterns": [],
            "pattern_significance": {},
            "anomaly_patterns": [],
            "behavioral_insights": []
        }
        
        # ç®€å•çš„æ–‡æœ¬è§£æé€»è¾‘
        lines = llm_response.split('\n')
        current_section = None
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # è¯†åˆ«æ®µè½
            if "æ¨¡å¼" in line and "ï¼š" in line:
                current_section = "identified_patterns"
            elif "å¼‚å¸¸" in line and "ï¼š" in line:
                current_section = "anomaly_patterns"
            elif "è¡Œä¸º" in line and "ï¼š" in line:
                current_section = "behavioral_insights"
            elif line.startswith('-') or line.startswith('â€¢'):
                # æå–å†…å®¹
                content = line.lstrip('-â€¢ ').strip()
                if content and current_section:
                    if current_section == "identified_patterns":
                        patterns["identified_patterns"].append(content)
                    elif current_section == "anomaly_patterns":
                        patterns["anomaly_patterns"].append(content)
                    elif current_section == "behavioral_insights":
                        patterns["behavioral_insights"].append(content)
        
        return patterns
    
    def _basic_pattern_analysis(self, responses: List[UserResponse]) -> Dict[str, Any]:
        """åŸºç¡€æ¨¡å¼åˆ†æ"""
        patterns = {
            "identified_patterns": [],
            "pattern_significance": {},
            "anomaly_patterns": [],
            "behavioral_insights": []
        }
        
        if not responses:
            return patterns
        
        # æ—¶é—´æ¨¡å¼åˆ†æ
        timestamps = [r.timestamp for r in responses if r.timestamp]
        if len(timestamps) > 1:
            time_diffs = []
            for i in range(1, len(timestamps)):
                diff = (timestamps[i] - timestamps[i-1]).total_seconds()
                time_diffs.append(diff)
            
            avg_time_diff = sum(time_diffs) / len(time_diffs)
            if avg_time_diff < 10:
                patterns["identified_patterns"].append("ç”¨æˆ·å›ç­”é€Ÿåº¦è¾ƒå¿«ï¼Œå¯èƒ½å¯¹é—®å·å†…å®¹ç†Ÿæ‚‰")
            elif avg_time_diff > 60:
                patterns["identified_patterns"].append("ç”¨æˆ·å›ç­”é€Ÿåº¦è¾ƒæ…¢ï¼Œå¯èƒ½éœ€è¦æ›´å¤šæ—¶é—´æ€è€ƒ")
        
        # ç­”æ¡ˆä¸€è‡´æ€§åˆ†æ
        answer_types = [type(r.answer).__name__ for r in responses]
        type_counter = Counter(answer_types)
        if len(type_counter) == 1:
            patterns["identified_patterns"].append("æ‰€æœ‰é—®é¢˜ä½¿ç”¨ç›¸åŒç±»å‹çš„ç­”æ¡ˆ")
        
        # å¼‚å¸¸æ¨¡å¼æ£€æµ‹
        for response in responses:
            if isinstance(response.answer, str) and len(response.answer) > 100:
                patterns["anomaly_patterns"].append(f"é—®é¢˜ {response.question_id} çš„å›ç­”å¼‚å¸¸é•¿")
            elif isinstance(response.answer, str) and len(response.answer) < 2:
                patterns["anomaly_patterns"].append(f"é—®é¢˜ {response.question_id} çš„å›ç­”å¼‚å¸¸çŸ­")
        
        return patterns
    
    def _assess_data_quality(self, responses: List[UserResponse]) -> Dict[str, Any]:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        quality_metrics = {
            "completeness": 0.0,
            "consistency": 0.0,
            "validity": 0.0,
            "timeliness": 0.0,
            "overall_score": 0.0,
            "quality_issues": []
        }
        
        if not responses:
            return quality_metrics
        
        # å®Œæ•´æ€§è¯„ä¼°
        total_questions = len(set(r.question_id for r in responses))
        unique_responses = len(set(r.question_id for r in responses))
        quality_metrics["completeness"] = unique_responses / total_questions if total_questions > 0 else 0.0
        
        # ä¸€è‡´æ€§è¯„ä¼°
        consistency_score = 0.0
        consistency_checks = 0
        
        for response in responses:
            if isinstance(response.answer, str):
                # æ£€æŸ¥ç­”æ¡ˆé•¿åº¦ä¸€è‡´æ€§
                if 2 <= len(response.answer) <= 100:
                    consistency_score += 1.0
                consistency_checks += 1
        
        if consistency_checks > 0:
            quality_metrics["consistency"] = consistency_score / consistency_checks
        
        # æœ‰æ•ˆæ€§è¯„ä¼°
        valid_responses = 0
        for response in responses:
            if response.answer and response.answer != "":
                valid_responses += 1
        
        quality_metrics["validity"] = valid_responses / len(responses) if responses else 0.0
        
        # åŠæ—¶æ€§è¯„ä¼°
        if len(responses) > 1:
            timestamps = [r.timestamp for r in responses if r.timestamp]
            if timestamps:
                time_span = (max(timestamps) - min(timestamps)).total_seconds()
                if time_span < 300:  # 5åˆ†é’Ÿå†…å®Œæˆ
                    quality_metrics["timeliness"] = 1.0
                elif time_span < 1800:  # 30åˆ†é’Ÿå†…å®Œæˆ
                    quality_metrics["timeliness"] = 0.8
                elif time_span < 3600:  # 1å°æ—¶å†…å®Œæˆ
                    quality_metrics["timeliness"] = 0.6
                else:
                    quality_metrics["timeliness"] = 0.4
        
        # è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
        weights = {"completeness": 0.3, "consistency": 0.25, "validity": 0.25, "timeliness": 0.2}
        overall_score = sum(quality_metrics[metric] * weights[metric] for metric in weights.keys())
        quality_metrics["overall_score"] = overall_score
        
        # è¯†åˆ«è´¨é‡é—®é¢˜
        if quality_metrics["completeness"] < 0.8:
            quality_metrics["quality_issues"].append("é—®å·å®Œæˆåº¦è¾ƒä½")
        if quality_metrics["consistency"] < 0.7:
            quality_metrics["quality_issues"].append("ç­”æ¡ˆä¸€è‡´æ€§è¾ƒå·®")
        if quality_metrics["validity"] < 0.9:
            quality_metrics["quality_issues"].append("å­˜åœ¨æ— æ•ˆç­”æ¡ˆ")
        if quality_metrics["timeliness"] < 0.6:
            quality_metrics["quality_issues"].append("å®Œæˆæ—¶é—´è¿‡é•¿")
        
        return quality_metrics
    
    async def _discover_insights(self, responses: List[UserResponse], 
                                questionnaire: Optional[Questionnaire]) -> List[Dict[str, Any]]:
        """å‘ç°æ•°æ®æ´å¯Ÿ"""
        logger.info(f"ğŸ’¡ {self.name} å¼€å§‹æ´å¯Ÿå‘ç°")
        
        try:
            # è·å–æ´å¯Ÿåˆ†ææç¤ºè¯
            q_dict = questionnaire.to_dict() if hasattr(questionnaire, "to_dict") else (questionnaire or {})
            prompt = AnalysisPrompts.data_analysis_prompt(
                responses=[r.to_dict() for r in responses],
                questionnaire=q_dict
            )
            
            # è°ƒç”¨LLMè¿›è¡Œæ´å¯Ÿåˆ†æ
            llm_response = await self.call_llm(prompt)
            
            # è§£ææ´å¯Ÿç»“æœ
            insights = self._parse_insights(llm_response)
            
            # å¦‚æœæ²¡æœ‰ç”Ÿæˆæ´å¯Ÿï¼Œä½¿ç”¨åŸºç¡€æ´å¯Ÿ
            if not insights:
                insights = self._generate_basic_insights(responses, questionnaire)
            
            logger.info(f"âœ… æ´å¯Ÿå‘ç°å®Œæˆ: {len(insights)} ä¸ªæ´å¯Ÿ")
            return insights
            
        except Exception as e:
            logger.error(f"âŒ æ´å¯Ÿå‘ç°å¤±è´¥: {e}")
            return self._generate_basic_insights(responses, questionnaire)
    
    def _parse_insights(self, llm_response: str) -> List[Dict[str, Any]]:
        """è§£æLLMç”Ÿæˆçš„æ´å¯Ÿ"""
        insights = []
        
        # ç®€å•çš„æ–‡æœ¬è§£æé€»è¾‘
        lines = llm_response.split('\n')
        current_insight = {}
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if line.startswith('æ´å¯Ÿ') or line.startswith('å‘ç°') or line.startswith('å…³é”®'):
                if current_insight:
                    insights.append(current_insight)
                current_insight = {"title": line, "description": "", "significance": "medium"}
            elif line.startswith('-') or line.startswith('â€¢'):
                if current_insight:
                    current_insight["description"] = line.lstrip('-â€¢ ').strip()
            elif "é‡è¦æ€§" in line or "æ„ä¹‰" in line:
                if current_insight:
                    if "é«˜" in line or "é‡è¦" in line:
                        current_insight["significance"] = "high"
                    elif "ä½" in line or "ä¸€èˆ¬" in line:
                        current_insight["significance"] = "low"
        
        # æ·»åŠ æœ€åä¸€ä¸ªæ´å¯Ÿ
        if current_insight:
            insights.append(current_insight)
        
        return insights
    
    def _generate_basic_insights(self, responses: List[UserResponse], 
                                questionnaire: Optional[Questionnaire]) -> List[Dict[str, Any]]:
        """ç”ŸæˆåŸºç¡€æ´å¯Ÿ"""
        insights = []
        
        if not responses:
            return insights
        
        # æ´å¯Ÿ1ï¼šå›ç­”æ¨¡å¼
        response_count = len(responses)
        if response_count > 10:
            insights.append({
                "title": "é—®å·å‚ä¸åº¦é«˜",
                "description": f"ç”¨æˆ·å®Œæˆäº†{response_count}ä¸ªé—®é¢˜çš„å›ç­”ï¼Œå‚ä¸åº¦è¾ƒé«˜",
                "significance": "medium"
            })
        
        # æ´å¯Ÿ2ï¼šæ—¶é—´æ¨¡å¼
        timestamps = [r.timestamp for r in responses if r.timestamp]
        if len(timestamps) > 1:
            time_span = (max(timestamps) - min(timestamps)).total_seconds()
            if time_span < 300:
                insights.append({
                    "title": "å¿«é€Ÿå®Œæˆæ¨¡å¼",
                    "description": "ç”¨æˆ·åœ¨5åˆ†é’Ÿå†…å®Œæˆé—®å·ï¼Œå¯èƒ½å¯¹å†…å®¹ç†Ÿæ‚‰æˆ–æ€¥äºå®Œæˆ",
                    "significance": "medium"
                })
        
        # æ´å¯Ÿ3ï¼šç­”æ¡ˆç±»å‹åˆ†å¸ƒ
        answer_types = [type(r.answer).__name__ for r in responses]
        type_counter = Counter(answer_types)
        if len(type_counter) == 1:
            insights.append({
                "title": "ç­”æ¡ˆç±»å‹ä¸€è‡´",
                "description": "æ‰€æœ‰é—®é¢˜ä½¿ç”¨ç›¸åŒç±»å‹çš„ç­”æ¡ˆï¼Œå¯èƒ½å½±å“æ•°æ®å¤šæ ·æ€§",
                "significance": "low"
            })
        
        # æ´å¯Ÿ4ï¼šæ•°æ®è´¨é‡
        valid_responses = sum(1 for r in responses if r.answer and r.answer != "")
        validity_rate = valid_responses / len(responses)
        if validity_rate < 0.9:
            insights.append({
                "title": "æ•°æ®è´¨é‡å…³æ³¨",
                "description": f"æ•°æ®æœ‰æ•ˆæ€§ä¸º{validity_rate:.1%}ï¼Œå­˜åœ¨æ— æ•ˆç­”æ¡ˆ",
                "significance": "high"
            })
        
        return insights
    
    def _generate_summary(self, basic_stats: Dict[str, Any], 
                         insights: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦"""
        summary = {
            "key_findings": [],
            "recommendations": [],
            "next_steps": []
        }
        
        # å…³é”®å‘ç°
        if basic_stats["total_responses"] > 0:
            summary["key_findings"].append(f"å…±æ”¶é›†åˆ°{basic_stats['total_responses']}ä¸ªå›ç­”")
        
        if basic_stats.get("completion_time"):
            summary["key_findings"].append(f"å¹³å‡å®Œæˆæ—¶é—´: {basic_stats['completion_time']:.1f}ç§’")
        
        # æ·»åŠ æ´å¯Ÿæ‘˜è¦
        high_significance_insights = [i for i in insights if i.get("significance") == "high"]
        if high_significance_insights:
            summary["key_findings"].append(f"å‘ç°{len(high_significance_insights)}ä¸ªé‡è¦æ´å¯Ÿ")
        
        # å»ºè®®
        if basic_stats.get("total_responses", 0) < 5:
            summary["recommendations"].append("å»ºè®®æ”¶é›†æ›´å¤šæ•°æ®ä»¥æé«˜åˆ†æå¯é æ€§")
        
        if insights:
            summary["recommendations"].append("å»ºè®®æ·±å…¥åˆ†æå‘ç°çš„æ´å¯Ÿ")
        
        # ä¸‹ä¸€æ­¥
        summary["next_steps"].append("ç»§ç»­ç›‘æ§æ•°æ®è´¨é‡")
        summary["next_steps"].append("å®šæœŸè¿›è¡Œæ•°æ®åˆ†æ")
        
        return summary
    
    def _create_error_analysis_result(self, error_message: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯åˆ†æç»“æœ"""
        return {
            "analysis_id": f"error_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "timestamp": datetime.now().isoformat(),
            "error": True,
            "error_message": error_message,
            "basic_statistics": {"total_responses": 0},
            "category_analysis": {},
            "pattern_analysis": {},
            "data_quality": {"overall_score": 0.0},
            "insights": [],
            "summary": {"key_findings": [], "recommendations": [], "next_steps": []}
        }
    
    def get_analysis_history(self) -> List[Dict[str, Any]]:
        """è·å–åˆ†æå†å²"""
        return self.analysis_history
    
    def export_analysis_report(self, analysis_result: Dict[str, Any], 
                              format: str = "json") -> str:
        """å¯¼å‡ºåˆ†ææŠ¥å‘Š"""
        if format.lower() == "json":
            return json.dumps(analysis_result, ensure_ascii=False, indent=2)
        elif format.lower() == "text":
            return self._format_text_report(analysis_result)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¯¼å‡ºæ ¼å¼: {format}")
    
    def _format_text_report(self, analysis_result: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–æ–‡æœ¬æŠ¥å‘Š"""
        report = f"é—®å·æ•°æ®åˆ†ææŠ¥å‘Š\n"
        report += f"=" * 50 + "\n\n"
        
        report += f"åˆ†æID: {analysis_result.get('analysis_id', 'N/A')}\n"
        report += f"åˆ†ææ—¶é—´: {analysis_result.get('timestamp', 'N/A')}\n"
        report += f"å›ç­”æ•°é‡: {analysis_result.get('responses_count', 0)}\n\n"
        
        # åŸºç¡€ç»Ÿè®¡
        basic_stats = analysis_result.get('basic_statistics', {})
        report += f"åŸºç¡€ç»Ÿè®¡:\n"
        report += f"- æ€»å›ç­”æ•°: {basic_stats.get('total_responses', 0)}\n"
        report += f"- å”¯ä¸€é—®é¢˜æ•°: {basic_stats.get('unique_questions', 0)}\n"
        if basic_stats.get('completion_time'):
            report += f"- å®Œæˆæ—¶é—´: {basic_stats.get('completion_time'):.1f}ç§’\n"
        report += "\n"
        
        # æ•°æ®è´¨é‡
        quality = analysis_result.get('data_quality', {})
        report += f"æ•°æ®è´¨é‡è¯„ä¼°:\n"
        report += f"- æ€»ä½“è¯„åˆ†: {quality.get('overall_score', 0):.2f}\n"
        report += f"- å®Œæ•´æ€§: {quality.get('completeness', 0):.2f}\n"
        report += f"- ä¸€è‡´æ€§: {quality.get('consistency', 0):.2f}\n"
        report += f"- æœ‰æ•ˆæ€§: {quality.get('validity', 0):.2f}\n"
        report += f"- åŠæ—¶æ€§: {quality.get('timeliness', 0):.2f}\n\n"
        
        # æ´å¯Ÿ
        insights = analysis_result.get('insights', [])
        if insights:
            report += f"å…³é”®æ´å¯Ÿ:\n"
            for i, insight in enumerate(insights, 1):
                report += f"{i}. {insight.get('title', 'N/A')}\n"
                report += f"   {insight.get('description', 'N/A')}\n"
                report += f"   é‡è¦æ€§: {insight.get('significance', 'N/A')}\n\n"
        
        # æ‘˜è¦
        summary = analysis_result.get('summary', {})
        if summary.get('key_findings'):
            report += f"ä¸»è¦å‘ç°:\n"
            for finding in summary['key_findings']:
                report += f"- {finding}\n"
            report += "\n"
        
        if summary.get('recommendations'):
            report += f"å»ºè®®:\n"
            for rec in summary['recommendations']:
                report += f"- {rec}\n"
            report += "\n"
        
        return report

if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®åˆ†ææ™ºèƒ½ä½“
    print("=== æ•°æ®åˆ†ææ™ºèƒ½ä½“æµ‹è¯• ===")
    
    # åˆ›å»ºæ™ºèƒ½ä½“
    analyzer = DataAnalyzerAgent()
    print(f"æ™ºèƒ½ä½“åˆ›å»ºæˆåŠŸ: {analyzer}")
    
    # æµ‹è¯•æ•°æ®åˆ†æ
    from ..models.questionnaire import UserResponse
    
    # æ¨¡æ‹Ÿç”¨æˆ·å›ç­”
    test_responses = [
        UserResponse("q1", "å¼ ä¸‰"),
        UserResponse("q2", "1"),
        UserResponse("q3", "55"),
        UserResponse("q4", "175"),
        UserResponse("q5", "70")
    ]
    
    import asyncio
    
    async def test_analysis():
        analysis_result = await analyzer.analyze_data(test_responses)
        print(f"æ•°æ®åˆ†æå®Œæˆ")
        print(f"å›ç­”æ•°é‡: {analysis_result.get('responses_count', 0)}")
        print(f"æ´å¯Ÿæ•°é‡: {len(analysis_result.get('insights', []))}")
        print(f"æ•°æ®è´¨é‡è¯„åˆ†: {analysis_result.get('data_quality', {}).get('overall_score', 0):.2f}")
        
        # å¯¼å‡ºæŠ¥å‘Š
        text_report = analyzer.export_analysis_report(analysis_result, "text")
        print(f"\næ–‡æœ¬æŠ¥å‘Š:\n{text_report[:500]}...")
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_analysis())
    
    print("âœ… æ•°æ®åˆ†ææ™ºèƒ½ä½“æµ‹è¯•å®Œæˆ")
